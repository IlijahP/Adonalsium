{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "gn57MbWfMKty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch pandas\n",
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "B12nW7KfMMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the book"
      ],
      "metadata": {
        "id": "mzTEdR4TNICB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URLs for the data\n",
        "book_url = 'https://gist.githubusercontent.com/JakeTurner616/7c5bcb37d861ba6fdb66d12a9bb9a084/raw/63d14a3aeed74a23b7aa1dba263b42d7c2b8676e/book.txt'  # Change to your book's URL\n",
        "\n",
        "# Download the book\n",
        "book_response = requests.get(book_url)\n",
        "book_text = book_response.text\n",
        "\n",
        "# Save the book text to a file (optional, you can also directly use `book_text` for training)\n",
        "with open('reverted_book.txt', 'w') as f:\n",
        "    f.write(book_text)\n",
        "\n",
        "print(\"Downloaded and saved the book text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RabOeSHqMV16",
        "outputId": "0aae10a8-4a90-46a3-bd6b-ae67c74eb947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and saved the book text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "LyJdxbo9Mn3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "model_name = 'EleutherAI/gpt-neo-2.7B'  # Example: Using a 2.7B parameter model\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-Neo also uses the eos token as a pad token\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"reverted_book.txt\",\n",
        "    block_size=128  # Adjust block size based on the available memory\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,  # Might need adjustment based on GPU memory\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "uG9wtYSZMpny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text"
      ],
      "metadata": {
        "id": "EYUCIJeiMrcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "def generate_text(prompt, model_path='./results/checkpoint-40000/'):\n",
        "    # Load the model and tokenizer based on the saved checkpoint\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "    # Create a text generation pipeline using the loaded model and tokenizer\n",
        "    text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "    # Generate text based on the provided prompt\n",
        "    generated_texts = text_generator(\n",
        "        prompt,\n",
        "        max_length=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.85,\n",
        "        repetition_penalty=1.3,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Print the generated texts\n",
        "    for generated in generated_texts:\n",
        "        print(generated['generated_text'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prompt = \"She fell to the ground dead\"\n",
        "    generate_text(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztvfMWLuMssr",
        "outputId": "e5fc96ce-61ec-4873-9124-766b1021a1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She fell to the ground dead, and it was a miracle she survived. Her hands were covered in blood, her feet scratched by something sharp.\n",
            "  Mistings are supposed not as dangerous as you think, Vin thought. She pulled back against the wall beside Elend. He had fallen unconscious there too. How did he survive? What if his wounds weren’t so bad that they made him sick? It didn't matter—he would die soon anyway. The only way out of this place was for them all together into one....\n",
            " Something crashed through Kelsier on top of Yomen. A man with dark brown skin. Like Ruin himself, but different. Marsh looked like someone else from what could be seen up above\n"
          ]
        }
      ]
    }
  ]
}