{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakeTurner616/Adonalsium/blob/LLM/trainandgenerate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn57MbWfMKty"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B12nW7KfMMx1",
        "outputId": "a09d9dc1-a82b-45b1-de4d-239fa737ff94"
      },
      "outputs": [],
      "source": [
        "! pip install torch pandas\n",
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzTEdR4TNICB"
      },
      "source": [
        "Download the book"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RabOeSHqMV16",
        "outputId": "0aae10a8-4a90-46a3-bd6b-ae67c74eb947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded and saved the book text.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# URLs for the data\n",
        "book_url = 'https://gist.githubusercontent.com/JakeTurner616/7c5bcb37d861ba6fdb66d12a9bb9a084/raw/63d14a3aeed74a23b7aa1dba263b42d7c2b8676e/book.txt'\n",
        "\n",
        "# Download the book\n",
        "book_response = requests.get(book_url)\n",
        "book_text = book_response.text\n",
        "\n",
        "# Save the book text to a file (optional, you can also directly use `book_text` for training)\n",
        "with open('reverted_book.txt', 'w') as f:\n",
        "    f.write(book_text)\n",
        "\n",
        "print(\"Downloaded and saved the book text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyJdxbo9Mn3q"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uG9wtYSZMpny",
        "outputId": "ce9a8a5b-e7a6-4fc5-9e17-2bef35372280"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token for GPT-2 compatibility\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"reverted_book.txt\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUCIJeiMrcc"
      },
      "source": [
        "Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztvfMWLuMssr",
        "outputId": "e5fc96ce-61ec-4873-9124-766b1021a1da"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "\n",
        "def generate_text(prompt, model_path='./results/checkpoint-40000/'):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "    generated_texts = text_generator(\n",
        "        prompt,\n",
        "        max_length=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.85,\n",
        "        repetition_penalty=1.3,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "      )\n",
        "\n",
        "    for generated in generated_texts:\n",
        "        print(generated['generated_text'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prompt = \"She fell to the ground dead\"\n",
        "    generate_text(prompt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOF6t/0EqjJHeIv/toMarib",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1L0H7X5R5gws8LkN3U2sYvGO-CPF51Sf4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
